llama2-7b-pretrain2:
  dtype: fp16
  log_dir: "/public1/home/stu51205901008/training_logs/llama2-7b-base"
  learning_rate: 1e-5
  model_name: /public1/home/stu51205901008/chinese-alpaca-2-7b-hf
  output_dir: /public1/home/stu51205901008/llama2-7b-400w-4gpu
  deepspeed_config: configs/zero3_config_pretrain.json
  lr_scheduler_type: linear
  weight_decay: 0.0
  residual_dropout: 0.0
  max_length: 4096
  val_max_length: 4096
  use_flash_attention: false
  warmup_steps: 1024
  gradient_checkpointing: true
  gradient_accumulation_steps: 32
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  eval_steps: 4615
  save_steps: 4615
  num_train_epochs: 2
  save_total_limit: 4
  sort_by_length: false
  datasets:
    - minimath:
        val_split: 0.05
        max_val_set: 250
    - gpt4all:
        val_split: 0.01
        max_val_set: 250
    - grade_school_math_instructions:
        val_split: 0.05
        max_val_set: 250
    - oa_leet10k:
        val_split: 0.05
        max_val_set: 250
    - _mix_en:
        val_split: 0.05
        max_val_set: 2000
    - _mix_belle:
        val_split: 0.05
        max_val_set: 2000
    - _mix_zh_others:
        val_split: 0.05
        max_val_set: 2000
    - _gushi_chengyu_pre:
        val_split: 0
        max_val_set: 250

llama2-13b-pretrain2:
  dtype: fp16
  log_dir: "/public1/home/stu51205901008/training_logs/llama2-13b-base"
  learning_rate: 1e-5
  model_name: /public1/home/stu51205901008/llama2-13b/chinese-alpaca-2-13b
  output_dir: /public1/home/stu51205901008/llama2-13b-400w-4gpu
  deepspeed_config: configs/zero3_config_pretrain.json
  lr_scheduler_type: linear
  weight_decay: 0.0
  residual_dropout: 0.0
  max_length: 4096
  val_max_length: 4096
  use_flash_attention: false
  warmup_steps: 1024
  gradient_checkpointing: true
  gradient_accumulation_steps: 32
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  eval_steps: 4615
  save_steps: 4615
  num_train_epochs: 2
  save_total_limit: 4
  sort_by_length: false
  datasets:
    - minimath:
        val_split: 0.05
        max_val_set: 250
    - gpt4all:
        val_split: 0.01
        max_val_set: 250
    - grade_school_math_instructions:
        val_split: 0.05
        max_val_set: 250
    - oa_leet10k:
        val_split: 0.05
        max_val_set: 250
    - _mix_en:
        val_split: 0.05
        max_val_set: 2000
    - _mix_belle:
        val_split: 0.05
        max_val_set: 2000
    - _mix_zh_others:
        val_split: 0.05
        max_val_set: 2000
    - _gushi_chengyu_pre:
        val_split: 0
        max_val_set: 250

bellellama-7b-pretrain2:
  dtype: fp16
  log_dir: "/public1/home/stu51205901008/training_logs/baichuan-13b-base"
  learning_rate: 1e-5
  model_name: /public1/home/stu51205901008/Baichuan-13B-Base
  output_dir: /public1/home/stu51205901008/baichuan-13b-400w-test
  deepspeed_config: configs/zero3_config_pretrain.json
  lr_scheduler_type: linear
  weight_decay: 0.0
  residual_dropout: 0.0
  max_length: 2048
  val_max_length: 2048
  use_flash_attention: false
  warmup_steps: 1024
  gradient_checkpointing: true
  gradient_accumulation_steps: 4
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 2
  eval_steps: 4620
  save_steps: 4620
  num_train_epochs: 3
  save_total_limit: 6
  sort_by_length: false
  datasets:
    - minimath:
        val_split: 0.05
        max_val_set: 250
    - gpt4all:
        val_split: 0.01
        max_val_set: 250
    - grade_school_math_instructions:
        val_split: 0.05
        max_val_set: 250
    - oa_leet10k:
        val_split: 0.05
        max_val_set: 250
    - _mix_en:
        val_split: 0.05
        max_val_set: 2000
    - _mix_belle:
        val_split: 0.05
        max_val_set: 2000
    - _mix_zh_others:
        val_split: 0.05
        max_val_set: 2000
    - _gushi_chengyu_pre:
        val_split: 0
        max_val_set: 250

full_highquality_search_7_17:
  dtype: fp16
  log_dir: "full_highquality_search_7_17_13b"
  learning_rate: 1e-5
  model_name: /public1/home/stu51205901008/baichuan-13b-400w
  output_dir: /public1/home/stu51205901008/baichuan-13b-400w-hq-test
  deepspeed_config: configs/zero3_config_pretrain.json
  weight_decay: 0.0
  residual_dropout: 0.0
  max_length: 2048
  use_flash_attention: false
  lr_scheduler_type: cosine
  warmup_steps: 356
  gradient_checkpointing: true
  gradient_accumulation_steps: 16
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  eval_steps: 738
  save_steps: 738
  num_train_epochs: 3
  save_total_limit: 6
  sort_by_length: false
  use_system_prefix: true
  system_prefix: "你是一个人工智能助手，名字叫EduChat。\n- EduChat是一个由华东师范大学开发的对话式语言模型。\nEduChat的工具\n- Web search: Disable.\n- Calculators: Disable.\nEduChat的能力\n- Inner Thought: Disable.\n对话主题\n- General: Enable.\n- Psychology: Disable.\n- Socrates: Disable."
  datasets:
    - oasst_export:
        lang: "en,zh" # sft-8.0
        input_file_path: /public1/home/stu51205901008/data/oasst_ready.trees.jsonl
        val_split: 0.05
        max_val_set: 300
    - dolly15k:
        val_split: 0.05
        max_val_set: 300
    
    - _ruozhiba:
        val_split: 0.0
    
    - _sharegpt_format: #vicuna
        size: 80000
        val_split: 0.05
        max_val_set: 500

    # 加入的特色数据
    - _similar_question:
        val_split: 0.0
    - _open_domain_subject:
        val_split: 0.0
    - _empdia_ly_07_17:
        val_split: 0.0
    - _zuowen:
        val_split: 0.0
    # - _psy_generated_zyg_6_29:
    #     val_split: 0.0
    # - _psy_gpt4_wo_merge_format:
    #     val_split: 0.0
    # - _psy_gpt4_merge_format:
    #     val_split: 0.0
    # - _socrates_psy_wo_inner:
    #     val_split: 0.0
    - _socrates_teaching:
        val_split: 0.0
    # - _composition_hq:
        # val_split: 0.0
    - _search:
        val_split: 0.0


    # 人设数据
    - _ecnu_qa:
        val_split: 0.0
    - _honest:
        val_split: 0

    # 自己生成高质量对话数据
    - _mix_gpt_4:
        val_split: 0.05
        max_val_set: 250

    # 开源高质量数据
    - _lima_qa:
        val_split: 0.0
    - _lima_chat:
        val_split: 0.0
    - _brainstorming_en:
        val_split: 0.05
        max_val_set: 250
    - _brainstorming_zh:
        val_split: 0.05
        max_val_set: 250
    - _code_en:
        val_split: 0.05
        max_val_set: 250
    - _code_zh:
        val_split: 0.05
        max_val_set: 250
    - _complex_instruction_en:
        val_split: 0.05
        max_val_set: 250
    - _continue_en:
        val_split: 0.05
        max_val_set: 250
    - _continue_zh:
        val_split: 0.05
        max_val_set: 250
    - _harmless_en:
        val_split: 0.05
        max_val_set: 250
    - _harmless_zh:
        val_split: 0.05
        max_val_set: 250
    - _role_playing_zh:
        val_split: 0.05
        max_val_set: 250
    - _switching_zh:
        val_split: 0.05
        max_val_set: 250
    - _writing_zh:
        val_split: 0.05
        max_val_set: 250
    - _complex_instruction_zh:
        val_split: 0.05
        max_val_set: 250
    
    - _role_playing_en:
        val_split: 0.05
        max_val_set: 250
    - _switching_en:
        val_split: 0.05
        max_val_set: 250
    - _writing_en:
        val_split: 0.05
        max_val_set: 250

full_highquality_search_6_29:
  dtype: fp16
  log_dir: "full_highquality_search_6_29_13b"
  learning_rate: 1e-5
  model_name: /data/ecnu/belle-13b-ext-oa
  output_dir: full_highquality_search_6_29
  deepspeed_config: configs/zero3_config_pretrain.json
  weight_decay: 0.0
  residual_dropout: 0.0
  max_length: 2048
  use_flash_attention: false
  lr_scheduler_type: linear
  warmup_steps: 356
  gradient_checkpointing: true
  gradient_accumulation_steps: 8
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  eval_steps: 738
  save_steps: 738
  num_train_epochs: 2
  save_total_limit: 6
  sort_by_length: false
  use_system_prefix: true
  system_prefix: "你是一个人工智能助手，名字叫EduChat。\n- EduChat是一个由华东师范大学开发的对话式语言模型。\nEduChat的工具\n- Web search: Disable.\n- Calculators: Disable.\nEduChat的能力\n- Inner Thought: Disable.\n对话主题\n- General: Enable.\n- Psychology: Disable.\n- Socrates: Disable."
  datasets:
    - oasst_export:
        lang: "en,zh" # sft-8.0
        input_file_path: /public1/home/stu51205901008/data/oasst_ready.trees.jsonl
        val_split: 0.05
        max_val_set: 300
    - dolly15k:
        val_split: 0.05
        max_val_set: 300
    
    - _ruozhiba:
        val_split: 0.0
    
    - _sharegpt_format: #vicuna
        size: 80000
        val_split: 0.05
        max_val_set: 500

    # 加入的特色数据
    - _similar_question:
        val_split: 0.0
    - _open_domain_subject:
        val_split: 0.0
    - _psy_generated_zyg_6_29:
        val_split: 0.0
    - _psy_gpt4_wo_merge_format:
        val_split: 0.0
    - _psy_gpt4_merge_format:
        val_split: 0.0
    - _socrates_psy_wo_inner:
        val_split: 0.0
    - _socrates_teaching:
        val_split: 0.0
    - _composition_hq:
        val_split: 0.0
    - _search:
        val_split: 0.0


    # 人设数据
    - _ecnu_qa:
        val_split: 0.0
    - _honest:
        val_split: 0

    # 自己生成高质量对话数据
    - _mix_gpt_4:
        val_split: 0.05
        max_val_set: 250

    # 开源高质量数据
    - _lima_qa:
        val_split: 0.0
    - _lima_chat:
        val_split: 0.0
    - _brainstorming_en:
        val_split: 0.05
        max_val_set: 250
    - _code_en:
        val_split: 0.05
        max_val_set: 250
    - _complex_instruction_en:
        val_split: 0.05
        max_val_set: 250
    - _continue_en:
        val_split: 0.05
        max_val_set: 250
    - _harmless_en:
        val_split: 0.05
        max_val_set: 250
    - _role_playing_zh:
        val_split: 0.05
        max_val_set: 250
    - _switching_zh:
        val_split: 0.05
        max_val_set: 250
    - _writing_zh:
        val_split: 0.05
        max_val_set: 250
    - _brainstorming_zh:
        val_split: 0.05
        max_val_set: 250
    - _code_zh:
        val_split: 0.05
        max_val_set: 250
    - _complex_instruction_zh:
        val_split: 0.05
        max_val_set: 250
    - _continue_zh:
        val_split: 0.05
        max_val_set: 250
    - _harmless_zh:
        val_split: 0.05
        max_val_set: 250
    - _role_playing_en:
        val_split: 0.05
        max_val_set: 250
    - _switching_en:
        val_split: 0.05
        max_val_set: 250
    - _writing_en:
        val_split: 0.05
        max_val_set: 250


full_highquality_ziya13b:
  dtype: fp16
  log_dir: "full_highquality_ziya13b"
  learning_rate: 1e-5
  model_name: /data/ecnu/Ziya-LLaMA-13B-Pretrained
  output_dir: full_highquality_ziya13b
  deepspeed_config: configs/zero3_config_pretrain.json
  weight_decay: 0.0
  residual_dropout: 0.0
  max_length: 2048
  use_flash_attention: false
  lr_scheduler_type: linear
  warmup_steps: 512
  gradient_checkpointing: true
  gradient_accumulation_steps: 4
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 4
  dataloader_num_workers: 8
  eval_steps: 1221
  save_steps: 2442
  num_train_epochs: 5
  save_total_limit: 6
  sort_by_length: false
  use_system_prefix: true
  system_prefix: "你是一个人工智能助手，名字叫EduChat。\n- EduChat是一个由华东师范大学开发的对话式语言模型。\nEduChat的工具\n- Web search: Disable.\n- Calculators: Disable.\n对话主题\n- General: Enable.\n- Psychology: Disable.\n- Socrates: Disable."
  datasets:
    - oasst_export:
        lang: "en,zh" # sft-8.0
        input_file_path: /public1/home/stu51205901008/data/oasst_ready.trees.jsonl
        val_split: 0.05
        max_val_set: 300
    - dolly15k:
        val_split: 0.05
        max_val_set: 300
    - _ruozhiba:
        val_split: 0.0
    
    - _sharegpt_format: #vicuna
        size: 80000
        val_split: 0.05
        max_val_set: 500
    # 加入的特色数据
    - _similar_question:
        val_split: 0.0
    - _open_domain_subject:
        val_split: 0.0
    # - _psy_diagnose:
    #     val_split: 0.0
    # - _socrates_psy:
    #     val_split: 0.0
    - _psy_diagnose_wo_inner:
        val_split: 0.0
    - _socrates_psy_wo_inner:
        val_split: 0.0
    - _socrates_teaching:
        val_split: 0.0
    - _composition_hq:
        val_split: 0.0

    # 人设数据
    - _ecnuqa:
        val_split: 0.0
    - _honest:
        val_split: 0

    # 自己生成高质量对话数据
    - _mix_gpt_4:
        val_split: 0.05
        max_val_set: 250

    # 开源高质量数据
    - _lima_qa:
        val_split: 0.0
    - _lima_chat:
        val_split: 0.0
    - _brainstorming_en:
        val_split: 0.05
        max_val_set: 250
    - _code_en:
        val_split: 0.05
        max_val_set: 250
    - _complex_instruction_en:
        val_split: 0.05
        max_val_set: 250
    - _continue_en:
        val_split: 0.05
        max_val_set: 250
    - _harmless_en:
        val_split: 0.05
        max_val_set: 250
    - _role_playing_zh:
        val_split: 0.05
        max_val_set: 250
    - _switching_zh:
        val_split: 0.05
        max_val_set: 250
    - _writing_zh:
        val_split: 0.05
        max_val_set: 250
    - _brainstorming_zh:
        val_split: 0.05
        max_val_set: 250
    - _code_zh:
        val_split: 0.05
        max_val_set: 250
    - _complex_instruction_zh:
        val_split: 0.05
        max_val_set: 250
    - _continue_zh:
        val_split: 0.05
        max_val_set: 250
    - _harmless_zh:
        val_split: 0.05
        max_val_set: 250
    - _role_playing_en:
        val_split: 0.05
        max_val_set: 250
    - _switching_en:
        val_split: 0.05
        max_val_set: 250
    - _writing_en:
        val_split: 0.05
        max_val_set: 250
    


full_highquality:
  dtype: fp16
  log_dir: "full_highquality"
  learning_rate: 1e-5
  model_name: /home/ubuntu/openassistant/model/model_training/llama_model_7b_p2/
  output_dir: full_highquality
  deepspeed_config: configs/zero3_config_pretrain.json
  weight_decay: 0.0
  residual_dropout: 0.0
  max_length: 2048
  use_flash_attention: false
  lr_scheduler_type: linear
  warmup_steps: 356
  gradient_checkpointing: true
  gradient_accumulation_steps: 4
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 4
  eval_steps: 717
  save_steps: 717
  num_train_epochs: 5
  save_total_limit: 6
  sort_by_length: false
  use_system_prefix: true
  system_prefix: "你是一个人工智能助手，名字叫EduChat。\n- EduChat是一个由华东师范大学开发的对话式语言模型。\nEduChat的工具\n- Web search: Disable.\n- Calculators: Disable.\n对话主题\n- General: Enable.\n- Psychology: Disable.\n- Socrates: Disable."
  datasets:
    - oasst_export:
        lang: "en,zh" # sft-8.0
        input_file_path: ../../../data/datasets/oasst_ready.trees.jsonl
        val_split: 0.05
        max_val_set: 300
    - dolly15k:
        val_split: 0.05
        max_val_set: 300
    

    - _ruozhiba:
        val_split: 0.0
    
    - _sharegpt_format: #vicuna
        size: 80000
        val_split: 0.05
        max_val_set: 500

    # 加入的特色数据
    - _similar_question:
        val_split: 0.0
    - _open_domain_subject:
        val_split: 0.0
    - _psy_diagnose:
        val_split: 0.0
    - _socrates_psy:
        val_split: 0.0
    - _socrates_teaching:
        val_split: 0.0
    - _composition_hq:
        val_split: 0.0

    # 人设数据
    - _ecnuqa:
        val_split: 0.0
    - _honest:
        val_split: 0

    # 自己生成高质量对话数据
    - _mix_gpt_4:
        val_split: 0.05
        max_val_set: 250

    # 开源高质量数据
    - _lima_qa:
        val_split: 0.0
    - _lima_chat:
        val_split: 0.0
    - _brainstorming_en:
        val_split: 0.05
        max_val_set: 250
    - _code_en:
        val_split: 0.05
        max_val_set: 250
    - _complex_instruction_en:
        val_split: 0.05
        max_val_set: 250
    - _continue_en:
        val_split: 0.05
        max_val_set: 250
    - _harmless_en:
        val_split: 0.05
        max_val_set: 250
    - _role_playing_zh:
        val_split: 0.05
        max_val_set: 250
    - _switching_zh:
        val_split: 0.05
        max_val_set: 250
    - _writing_zh:
        val_split: 0.05
        max_val_set: 250
    - _brainstorming_zh:
        val_split: 0.05
        max_val_set: 250
    - _code_zh:
        val_split: 0.05
        max_val_set: 250
    - _complex_instruction_zh:
        val_split: 0.05
        max_val_set: 250
    - _continue_zh:
        val_split: 0.05
        max_val_set: 250
    - _harmless_zh:
        val_split: 0.05
        max_val_set: 250
    - _role_playing_en:
        val_split: 0.05
        max_val_set: 250
    - _switching_en:
        val_split: 0.05
        max_val_set: 250
    - _writing_en:
        val_split: 0.05
        max_val_set: 250
    


test2_:
  dtype: fp16
  log_dir: "test2_"
  learning_rate: 1e-5
  model_name: /home/ubuntu/openassistant/model/model_training/test_/
  output_dir: test2_
  deepspeed_config: configs/zero3_config_pretrain.json
  weight_decay: 0.0
  residual_dropout: 0.0
  max_length: 2048
  use_flash_attention: false
  lr_scheduler_type: linear
  warmup_steps: 32
  gradient_checkpointing: true
  gradient_accumulation_steps: 4
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 4
  eval_steps: 717
  save_steps: 717
  num_train_epochs: 2
  save_total_limit: 5
  sort_by_length: false
  use_system_prefix: true
  system_prefix: "你是一个人工智能助手，名字叫EduChat。\n- EduChat是一个由华东师范大学开发的对话式语言模型。\nEduChat的工具\n- Web search: Disable.\n- Calculators: Disable.\n对话主题\n- General: Enable.\n- Psychology: Disable.\n- Socrates: Disable."
  datasets:
    - oasst_export:
        lang: "en,zh" # sft-8.0
        input_file_path: /home/ubuntu/openassistant/data/datasets/oasst_ready.trees.jsonl
        val_split: 0.05
        max_val_set: 300
    - dolly15k:
        val_split: 0.05
        max_val_set: 300
    
    # 注释原因：400w高质量数据加入了，驹哥加入训练
    # - _gushi_chengyu_pre:
    #     val_split: 0


    # 注释原因：400w高质量数据加入了，驹哥宇豪哥可加入训练
    # - wizardlm_70k:
    #     size: 40000
    #     val_split: 0.05
    #     max_val_set: 500
    
    - _sharegpt_format: #vicuna
        size: 80000
        val_split: 0.05
        max_val_set: 500

    # 加入的特色数据
    - _similar_question:
        val_split: 0.0
    - _open_domain_subject:
        val_split: 0.0
    - _psy_diagnose:
        val_split: 0.0
    - _socrates_psy:
        val_split: 0.0
    - _socrates_teaching:
        val_split: 0.0
    - _composition_hq:
        val_split: 0.0

    # 人设数据
    - _ecnuqa:
        val_split: 0.0
    - _honest:
        val_split: 0

    # 自己生成高质量对话数据
    - _mix_gpt_4:
        val_split: 0.05
        max_val_set: 250

    # 开源高质量数据
    - _lima_qa:
        val_split: 0.0
    - _lima_chat:
        val_split: 0.0
    - _brainstorming_en:
        val_split: 0.05
        max_val_set: 250
    - _code_en:
        val_split: 0.05
        max_val_set: 250
    - _complex_instruction_en:
        val_split: 0.05
        max_val_set: 250
    - _continue_en:
        val_split: 0.05
        max_val_set: 250
    - _harmless_en:
        val_split: 0.05
        max_val_set: 250
    - _role_playing_zh:
        val_split: 0.05
        max_val_set: 250
    - _switching_zh:
        val_split: 0.05
        max_val_set: 250
    - _writing_zh:
        val_split: 0.05
        max_val_set: 250
    - _brainstorming_zh:
        val_split: 0.05
        max_val_set: 250
    - _code_zh:
        val_split: 0.05
        max_val_set: 250
    - _complex_instruction_zh:
        val_split: 0.05
        max_val_set: 250
    - _continue_zh:
        val_split: 0.05
        max_val_set: 250
    - _harmless_zh:
        val_split: 0.05
        max_val_set: 250
    - _role_playing_en:
        val_split: 0.05
        max_val_set: 250
    - _switching_en:
        val_split: 0.05
        max_val_set: 250
    - _writing_en:
        val_split: 0.05
        max_val_set: 250
    



test_:
  dtype: fp16
  log_dir: "test_"
  learning_rate: 1e-5
  model_name: /home/hy/openassistant/model/model_training/llama_model_7b_p2/
  output_dir: test_
  deepspeed_config: configs/zero3_config_pretrain.json
  weight_decay: 0.0
  residual_dropout: 0.0
  max_length: 2048
  use_flash_attention: false
  lr_scheduler_type: linear
  warmup_steps: 32
  gradient_checkpointing: true
  gradient_accumulation_steps: 4
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 4
  eval_steps: 717
  save_steps: 717
  num_train_epochs: 3
  save_total_limit: 5
  sort_by_length: false
  use_system_prefix: true
  system_prefix: "你是一个人工智能助手，名字叫EduChat。\n- EduChat是一个由华东师范大学开发的对话式语言模型。\nEduChat的工具\n- Web search: Disable.\n- Calculators: Disable.\n对话主题\n- General: Enable.\n- Psychology: Disable.\n- Socrates: Disable."
  datasets:
    - oasst_export:
        lang: "en,zh" # sft-8.0
        input_file_path: ../../../data/datasets/oasst_ready.trees.jsonl
        val_split: 0.05
        max_val_set: 300
    - dolly15k:
        val_split: 0.05
        max_val_set: 300
    
    # 注释原因：400w高质量数据加入了，驹哥加入训练
    # - _gushi_chengyu_pre:
    #     val_split: 0


    # 注释原因：400w高质量数据加入了，驹哥宇豪哥可加入训练
    # - wizardlm_70k:
    #     size: 40000
    #     val_split: 0.05
    #     max_val_set: 500
    
    - _sharegpt_format: #vicuna
        size: 80000
        val_split: 0.05
        max_val_set: 500

    # 加入的特色数据
    - _similar_question:
        val_split: 0.0
    - _open_domain_subject:
        val_split: 0.0
    - _psy_diagnose:
        val_split: 0.0
    - _socrates_psy:
        val_split: 0.0
    - _socrates_teaching:
        val_split: 0.0
    - _composition_hq:
        val_split: 0.0

    # 人设数据
    - _ecnuqa:
        val_split: 0.0
    - _honest:
        val_split: 0

    # 自己生成高质量对话数据
    - _mix_gpt_4:
        val_split: 0.05
        max_val_set: 250

    # 开源高质量数据
    - _lima_qa:
        val_split: 0.0
    - _lima_chat:
        val_split: 0.0
    - _brainstorming_en:
        val_split: 0.05
        max_val_set: 250
    - _code_en:
        val_split: 0.05
        max_val_set: 250
    - _complex_instruction_en:
        val_split: 0.05
        max_val_set: 250
    - _continue_en:
        val_split: 0.05
        max_val_set: 250
    - _harmless_en:
        val_split: 0.05
        max_val_set: 250
    - _role_playing_zh:
        val_split: 0.05
        max_val_set: 250
    - _switching_zh:
        val_split: 0.05
        max_val_set: 250
    - _writing_zh:
        val_split: 0.05
        max_val_set: 250
    - _brainstorming_zh:
        val_split: 0.05
        max_val_set: 250
    - _code_zh:
        val_split: 0.05
        max_val_set: 250
    - _complex_instruction_zh:
        val_split: 0.05
        max_val_set: 250
    - _continue_zh:
        val_split: 0.05
        max_val_set: 250
    - _harmless_zh:
        val_split: 0.05
        max_val_set: 250
    - _role_playing_en:
        val_split: 0.05
        max_val_set: 250
    - _switching_en:
        val_split: 0.05
        max_val_set: 250
    - _writing_en:
        val_split: 0.05
        max_val_set: 250
    


bellellama-7b-instr_renshe_zuowen_xinli_subject_simquestion_continue:
  dtype: fp16
  log_dir: "bellellama-7b-instr_renshe_zuowen_xinli_subject_simquestio_continue"
  learning_rate: 1e-5
  model_name: /home/ubuntu/openassistant/model/model_training/llama_model_7b_instr_renshe_zuowen_xinli_subject_simquestio/
  output_dir: llama_model_7b_instr_renshe_zuowen_xinli_subject_simquestio_continue
  deepspeed_config: configs/zero3_config_pretrain2.json
  weight_decay: 0.0
  residual_dropout: 0.0
  max_length: 2048
  use_flash_attention: false
  lr_scheduler_type: cosine
  warmup_steps: 32
  gradient_checkpointing: true
  gradient_accumulation_steps: 4
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 4
  eval_steps: 717
  save_steps: 717
  num_train_epochs: 3
  save_total_limit: 5
  sort_by_length: false
  datasets:
    - oasst_export:
        lang: "en,zh" # sft-8.0
        input_file_path: /home/ubuntu/openassistant/data/datasets/oasst_ready.trees.jsonl
        val_split: 0.05
        max_val_set: 300
    - dolly15k:
        val_split: 0.05
        max_val_set: 300
    
    # 注释原因：400w高质量数据加入了，驹哥加入训练
    # - _gushi_chengyu_pre:
    #     val_split: 0


    # 注释原因：400w高质量数据加入了，驹哥宇豪哥可加入训练
    # - wizardlm_70k:
    #     size: 40000
    #     val_split: 0.05
    #     max_val_set: 500
    
    - _sharegpt_format: #vicuna
        size: 80000
        val_split: 0.05
        max_val_set: 500

    # 加入的特色数据
    - _similar_question:
        val_split: 0.0
    - _open_domain_subject:
        val_split: 0.0
    - _psychology_instructs:
        val_split: 0.0
    - _compositions:
        val_split: 0.0
    - _psychology_chat:
        val_split: 0.0

    # 人设数据
    - _ecnuqa:
        val_split: 0.0
    - _honest:
        val_split: 0

    # 自己生成高质量对话数据
    - _mix_gpt_4:
        val_split: 0.05
        max_val_set: 250

    # 开源高质量数据
    - _lima_qa:
        val_split: 0.0
    - _lima_chat:
        val_split: 0.0
    - _brainstorming_en:
        val_split: 0.05
        max_val_set: 250
    - _code_en:
        val_split: 0.05
        max_val_set: 250
    - _complex_instruction_en:
        val_split: 0.05
        max_val_set: 250
    - _continue_en:
        val_split: 0.05
        max_val_set: 250
    - _harmless_en:
        val_split: 0.05
        max_val_set: 250
    - _role_playing_zh:
        val_split: 0.05
        max_val_set: 250
    - _switching_zh:
        val_split: 0.05
        max_val_set: 250
    - _writing_zh:
        val_split: 0.05
        max_val_set: 250
    - _brainstorming_zh:
        val_split: 0.05
        max_val_set: 250
    - _code_zh:
        val_split: 0.05
        max_val_set: 250
    - _complex_instruction_zh:
        val_split: 0.05
        max_val_set: 250
    - _continue_zh:
        val_split: 0.05
        max_val_set: 250
    - _harmless_zh:
        val_split: 0.05
        max_val_set: 250
    - _role_playing_en:
        val_split: 0.05
        max_val_set: 250
    - _switching_en:
        val_split: 0.05
        max_val_set: 250
    - _writing_en:
        val_split: 0.05
        max_val_set: 250


bellellama-7b-instr_renshe_zuowen_xinli_subject_simquestion:
  dtype: fp16
  log_dir: "bellellama-7b-instr_renshe_zuowen_xinli_subject_simquestio"
  learning_rate: 1e-5
  model_name: /home/ubuntu/openassistant/model/model_training/llama_model_7b_p2/
  output_dir: llama_model_7b_instr_renshe_zuowen_xinli_subject_simquestio
  deepspeed_config: configs/zero3_config_pretrain.json
  weight_decay: 0.0
  residual_dropout: 0.0
  max_length: 2048
  use_flash_attention: false
  lr_scheduler_type: linear
  warmup_steps: 256
  gradient_checkpointing: true
  gradient_accumulation_steps: 4
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 4
  eval_steps: 717
  save_steps: 717
  num_train_epochs: 3
  save_total_limit: 5
  sort_by_length: false
  datasets:
    - oasst_export:
        lang: "en,zh" # sft-8.0
        input_file_path: /home/ubuntu/openassistant/data/datasets/oasst_ready.trees.jsonl
        val_split: 0.05
        max_val_set: 300
    - dolly15k:
        val_split: 0.05
        max_val_set: 300
    
    # 注释原因：400w高质量数据加入了，驹哥加入训练
    # - _gushi_chengyu_pre:
    #     val_split: 0


    # 注释原因：400w高质量数据加入了，驹哥宇豪哥可加入训练
    # - wizardlm_70k:
    #     size: 40000
    #     val_split: 0.05
    #     max_val_set: 500
    
    - _sharegpt_format: #vicuna
        size: 80000
        val_split: 0.05
        max_val_set: 500

    # 加入的特色数据
    - _similar_question:
        val_split: 0.0
    - _open_domain_subject:
        val_split: 0.0
    - _compositions:
        val_split: 0.0
    - _psychology_chat:
        val_split: 0.0

    # 人设数据
    - _ecnuqa:
        val_split: 0.0
    - _honest:
        val_split: 0

    # 自己生成高质量对话数据
    - _mix_gpt_4:
        val_split: 0.05
        max_val_set: 250

    # 开源高质量数据
    - _lima_qa:
        val_split: 0.0
    - _lima_chat:
        val_split: 0.0
    - _brainstorming_en:
        val_split: 0.05
        max_val_set: 250
    - _code_en:
        val_split: 0.05
        max_val_set: 250
    - _complex_instruction_en:
        val_split: 0.05
        max_val_set: 250
    - _continue_en:
        val_split: 0.05
        max_val_set: 250
    - _harmless_en:
        val_split: 0.05
        max_val_set: 250
    - _role_playing_zh:
        val_split: 0.05
        max_val_set: 250
    - _switching_zh:
        val_split: 0.05
        max_val_set: 250
    - _writing_zh:
        val_split: 0.05
        max_val_set: 250
    - _brainstorming_zh:
        val_split: 0.05
        max_val_set: 250
    - _code_zh:
        val_split: 0.05
        max_val_set: 250
    - _complex_instruction_zh:
        val_split: 0.05
        max_val_set: 250
    - _continue_zh:
        val_split: 0.05
        max_val_set: 250
    - _harmless_zh:
        val_split: 0.05
        max_val_set: 250
    - _role_playing_en:
        val_split: 0.05
        max_val_set: 250
    - _switching_en:
        val_split: 0.05
        max_val_set: 250
    - _writing_en:
        val_split: 0.05
        max_val_set: 250


bellellama-7b-pretrain2:
  dtype: fp16
  log_dir: "/public1/home/stu51205901008/training_logs/baichuan-13b-base"
  learning_rate: 1e-5
  model_name: /public1/home/stu51205901008/Baichuan-13B-Base
  output_dir: /public1/home/stu51205901008/baichuan-13b-400w-test
  deepspeed_config: configs/zero3_config_pretrain.json
  lr_scheduler_type: linear
  weight_decay: 0.0
  residual_dropout: 0.0
  max_length: 2048
  val_max_length: 2048
  use_flash_attention: false
  warmup_steps: 1024
  gradient_checkpointing: true
  gradient_accumulation_steps: 4
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 2
  eval_steps: 4620
  save_steps: 4620
  num_train_epochs: 3
  save_total_limit: 6
  sort_by_length: false
  datasets:
    - minimath:
        val_split: 0.05
        max_val_set: 250
    - gpt4all:
        val_split: 0.01
        max_val_set: 250
    - grade_school_math_instructions:
        val_split: 0.05
        max_val_set: 250
    - oa_leet10k:
        val_split: 0.05
        max_val_set: 250
    - _mix_en:
        val_split: 0.05
        max_val_set: 2000
    - _mix_belle:
        val_split: 0.05
        max_val_set: 2000
    - _mix_zh_others:
        val_split: 0.05
        max_val_set: 2000
    - _gushi_chengyu_pre:
        val_split: 0
        max_val_set: 250



    

bellellama-7b-instr:
  dtype: fp16
  log_dir: "bellellama-7b-instr-log"
  learning_rate: 1e-5
  model_name: /home/ubuntu/openassistant/model/model_training/llama_model_7b/
  output_dir: llama_model_7b_instr
  deepspeed_config: configs/zero3_config_pretrain.json
  weight_decay: 0.0
  residual_dropout: 0.0
  max_length: 2048
  use_flash_attention: false
  lr_scheduler_type: cosine
  warmup_steps: 400
  gradient_checkpointing: true
  gradient_accumulation_steps: 4
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 4
  eval_steps: 400
  save_steps: 400
  num_train_epochs: 3
  save_total_limit: 10
  sort_by_length: false
  datasets:
    - oasst_export:
        lang: "en,zh" # sft-8.0
        input_file_path: /home/ubuntu/openassistant/data/datasets/2023-04-12_oasst_ready.trees.jsonl.gz
        val_split: 0.05
        max_val_set: 300
    - dolly15k:
        val_split: 0.05
        max_val_set: 300
    - wizardlm_70k:
        size: 40000
        val_split: 0.05
        max_val_set: 500
    - vicuna:
        size: 80000
        val_split: 0.05
        max_val_set: 500

    - _ecnuqa:
        val_split: 0.0
    - _honest:
        val_split: 0
    - _mix_gpt_4:
        val_split: 0.05
        max_val_set: 250

    - _brainstorming_en:
        val_split: 0.05
        max_val_set: 250
    - _code_en:
        val_split: 0.05
        max_val_set: 250
    - _complex_instruction_en:
        val_split: 0.05
        max_val_set: 250
    - _continue_en:
        val_split: 0.05
        max_val_set: 250
    - _harmless_en:
        val_split: 0.05
        max_val_set: 250
    - _role_playing_zh:
        val_split: 0.05
        max_val_set: 250
    - _switching_zh:
        val_split: 0.05
        max_val_set: 250
    - _writing_zh:
        val_split: 0.05
        max_val_set: 250
    - _brainstorming_zh:
        val_split: 0.05
        max_val_set: 250
    - _code_zh:
        val_split: 0.05
        max_val_set: 250
    - _complex_instruction_zh:
        val_split: 0.05
        max_val_set: 250
    - _continue_zh:
        val_split: 0.05
        max_val_set: 250
    - _harmless_zh:
        val_split: 0.05
        max_val_set: 250
    - _role_playing_en:
        val_split: 0.05
        max_val_set: 250
    - _switching_en:
        val_split: 0.05
        max_val_set: 250
    - _writing_en:
        val_split: 0.05
        max_val_set: 250

    

bellellama-7b-pretrain:
  dtype: fp16
  log_dir: "bellellama-7b-pretrain-log"
  learning_rate: 1e-5
  model_name: /home/ubuntu/belle-llama-7b-ext/
  output_dir: llama_model_7b
  deepspeed_config: configs/zero3_config_pretrain.json
  weight_decay: 0.0
  residual_dropout: 0.0
  max_length: 2048
  use_flash_attention: false
  warmup_steps: 1024
  gradient_checkpointing: true
  gradient_accumulation_steps: 4
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 4
  eval_steps: 3241
  save_steps: 3241
  num_train_epochs: 2
  save_total_limit: 11
  sort_by_length: false
  datasets:
    - minimath:
        val_split: 0.05
        max_val_set: 250
    - gpt4all:
        val_split: 0.01
        max_val_set: 250
    - grade_school_math_instructions:
        val_split: 0.05
        max_val_set: 250
    - oa_leet10k:
        val_split: 0.05
        max_val_set: 250
    - _alpaca:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/opensource_data/alpaca.jsonl
        max_val_set: 250
    - _belle:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/opensource_data/Belle.jsonl
        max_val_set: 250
    - _belle_dialog:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/opensource_data/BELLE_dialog.jsonl
        max_val_set: 250
    - _belle_generated_chat:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/opensource_data/BELLE_generated_chat.jsonl
        max_val_set: 250
    - _belle_math:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/opensource_data/BELLE_math.jsonl
        max_val_set: 250
    - _belle_train_2m:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/opensource_data/BELLE_train_2M.jsonl
        max_val_set: 250
    - _coig_multi_turn:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/opensource_data/COIG_multi_turn.jsonl
        max_val_set: 250
    - _coig_single:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/opensource_data/COIG_single.jsonl
        max_val_set: 250
    - _firefly:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/opensource_data/firefly.jsonl
        max_val_set: 250
    - _flan_cot:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/opensource_data/Flan_CoT.jsonl
        max_val_set: 250
    - _instinwild_ch:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/opensource_data/instinwild_ch.jsonl
        max_val_set: 250
    - _instinwild_en:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/opensource_data/instinwild_en.jsonl
        max_val_set: 250
    - _unnatural_instruction:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/opensource_data/Unnatural_Instruction.jsonl
        max_val_set: 250
    - _composition_review:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/composition_review.jsonl
        max_val_set: 250
    - _correct:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/correct.jsonl
        max_val_set: 250
    - _emotion_dialog:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/emotion_dialog.jsonl
        max_val_set: 250
    - _poem_generate:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/poem_generate.jsonl
        max_val_set: 250
    - _poem_transform:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/poem_transform.jsonl
        max_val_set: 250
    - _reading_comprehension_en:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/reading_comprehension_en.jsonl
        max_val_set: 250
    - _rewriting:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/rewriting.jsonl
        max_val_set: 250
    - _story_generate:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/story_generate.jsonl
        max_val_set: 250
    - _summary:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/summary.jsonl
        max_val_set: 250
    - _transstyle:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/TransStyle.jsonl
        max_val_set: 250
    - _writing:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/writing.jsonl
        max_val_set: 250
    - _writing_cot:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/writing_cot.jsonl
        max_val_set: 250
    - _chengyu_gushi:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/chengyu/chengyu_gushi.jsonl
        max_val_set: 250
    - _chengyu_jieshi:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/chengyu/chengyu_jieshi.jsonl
        max_val_set: 250
    - _chengyu_laiyuan:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/chengyu/chengyu_laiyuan.jsonl
        max_val_set: 250
    - _chengyu_liju:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/chengyu/chengyu_liju.jsonl
        max_val_set: 250
    - _chengyu_pinyin:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/chengyu/chengyu_pinyin.jsonl
        max_val_set: 250
    - _jieshi_chengyu:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/chengyu/jieshi_chengyu.jsonl
        max_val_set: 250
    - _biaoti_chaodai:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/gushi/biaoti_chaodai.jsonl
        max_val_set: 250
    - _biaoti_gushi:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/gushi/biaoti_gushi.jsonl
        max_val_set: 250
    - _biaoti_leixing:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/gushi/biaoti_leixing.jsonl
        max_val_set: 250
    - _biaoti_mingju:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/gushi/biaoti_mingju.jsonl
        max_val_set: 250
    - _biaoti_zuozhe:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/gushi/biaoti_zuozhe.jsonl
        max_val_set: 250
    - _gushi-beijing_yiwen:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/gushi/gushi-beijing_yiwen.jsonl
        max_val_set: 250
    - _gushi_beijing:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/gushi/gushi_beijing.jsonl
        max_val_set: 250
    - _gushi_biaoti:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/gushi/gushi_biaoti.jsonl
        max_val_set: 250
    - _gushi_chaodai:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/gushi/gushi_chaodai.jsonl
        max_val_set: 250
    - _gushi_gushi:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/gushi/gushi_gushi.jsonl
        max_val_set: 250
    - _gushi_leixing:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/gushi/gushi_leixing.jsonl
        max_val_set: 250
    - _gushi_mingju:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/gushi/gushi_mingju.jsonl
        max_val_set: 250
    - _gushi_shangxi:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/gushi/gushi_shangxi.jsonl
        max_val_set: 250
    - _gushi_yiwen:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/gushi/gushi_yiwen.jsonl
        max_val_set: 250
    - _gushi_zuozhe:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/gushi/gushi_zuozhe.jsonl
        max_val_set: 250
    - _leixing_biaoti:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/gushi/leixing_biaoti.jsonl
        max_val_set: 250
    - _leixing_gushi:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/gushi/leixing_gushi.jsonl
        max_val_set: 250
    - _leixing_zuozhe:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/gushi/leixing_zuozhe.jsonl
        max_val_set: 250
    - _mingju_biaoti-yuanwen:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/gushi/mingju_biaoti-yuanwen.jsonl
        max_val_set: 250
    - _mingju_zuozhe:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/gushi/mingju_zuozhe.jsonl
        max_val_set: 250
    - _shiren_chaodai:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/gushi/shiren_chaodai.jsonl
        max_val_set: 250
    - _shiren_jieshao:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/gushi/shiren_jieshao.jsonl
        max_val_set: 250
    - _zuozhe-leixing_biaoti:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/gushi/zuozhe-leixing_biaoti.jsonl
        max_val_set: 250
    - _zuozhe_gushi:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/gushi/zuozhe_gushi.jsonl
        max_val_set: 250
    - _zuozhe_leixing:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/gushi/zuozhe_leixing.jsonl
        max_val_set: 250
    - _zuozhe_mingju:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/gushi/zuozhe_mingju.jsonl
        max_val_set: 250
    - _daan_miyu:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/miyu/daan_miyu.jsonl
        max_val_set: 250
    - _miyu_daan:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/miyu/miyu_daan.jsonl
        max_val_set: 250


myllama-13b-pretrain:
  dtype: fp16
  log_dir: "myllama-13b-pretrain-log"
  learning_rate: 1e-5
  model_name: P01son/Linly-Chinese-LLaMA-13b-hf
  output_dir: llama_model_13b
  deepspeed_config: configs/zero3_config_pretrain.json
  weight_decay: 0.0
  residual_dropout: 0.0
  max_length: 2048
  use_flash_attention: false
  warmup_steps: 1024
  gradient_checkpointing: true
  gradient_accumulation_steps: 4
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 8
  eval_steps: 3241
  save_steps: 3241
  num_train_epochs: 2
  save_total_limit: 5
  sort_by_length: false
  datasets:
    - minimath:
        val_split: 0.05
        max_val_set: 250
    - gpt4all:
        val_split: 0.01
        max_val_set: 250
    - grade_school_math_instructions:
        val_split: 0.05
        max_val_set: 250
    - oa_leet10k:
        val_split: 0.05
        max_val_set: 250
    - _alpaca:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/opensource_data/alpaca.jsonl
        max_val_set: 250
    - _belle:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/opensource_data/Belle.jsonl
        max_val_set: 250
    - _belle_dialog:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/opensource_data/BELLE_dialog.jsonl
        max_val_set: 250
    - _belle_generated_chat:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/opensource_data/BELLE_generated_chat.jsonl
        max_val_set: 250
    - _belle_math:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/opensource_data/BELLE_math.jsonl
        max_val_set: 250
    - _belle_train_2m:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/opensource_data/BELLE_train_2M.jsonl
        max_val_set: 250
    - _coig_multi_turn:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/opensource_data/COIG_multi_turn.jsonl
        max_val_set: 250
    - _coig_single:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/opensource_data/COIG_single.jsonl
        max_val_set: 250
    - _firefly:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/opensource_data/firefly.jsonl
        max_val_set: 250
    - _flan_cot:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/opensource_data/Flan_CoT.jsonl
        max_val_set: 250
    - _instinwild_ch:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/opensource_data/instinwild_ch.jsonl
        max_val_set: 250
    - _instinwild_en:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/opensource_data/instinwild_en.jsonl
        max_val_set: 250
    - _unnatural_instruction:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/opensource_data/Unnatural_Instruction.jsonl
        max_val_set: 250
    - _composition_review:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/composition_review.jsonl
        max_val_set: 250
    - _correct:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/correct.jsonl
        max_val_set: 250
    - _emotion_dialog:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/emotion_dialog.jsonl
        max_val_set: 250
    - _poem_generate:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/poem_generate.jsonl
        max_val_set: 250
    - _poem_transform:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/poem_transform.jsonl
        max_val_set: 250
    - _reading_comprehension_en:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/reading_comprehension_en.jsonl
        max_val_set: 250
    - _rewriting:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/rewriting.jsonl
        max_val_set: 250
    - _story_generate:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/story_generate.jsonl
        max_val_set: 250
    - _summary:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/summary.jsonl
        max_val_set: 250
    - _transstyle:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/TransStyle.jsonl
        max_val_set: 250
    - _writing:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/writing.jsonl
        max_val_set: 250
    - _writing_cot:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/writing_cot.jsonl
        max_val_set: 250
    - _chengyu_gushi:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/chengyu/chengyu_gushi.jsonl
        max_val_set: 250
    - _chengyu_jieshi:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/chengyu/chengyu_jieshi.jsonl
        max_val_set: 250
    - _chengyu_laiyuan:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/chengyu/chengyu_laiyuan.jsonl
        max_val_set: 250
    - _chengyu_liju:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/chengyu/chengyu_liju.jsonl
        max_val_set: 250
    - _chengyu_pinyin:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/chengyu/chengyu_pinyin.jsonl
        max_val_set: 250
    - _jieshi_chengyu:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/chengyu/jieshi_chengyu.jsonl
        max_val_set: 250
    - _biaoti_chaodai:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/gushi/biaoti_chaodai.jsonl
        max_val_set: 250
    - _biaoti_gushi:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/gushi/biaoti_gushi.jsonl
        max_val_set: 250
    - _biaoti_leixing:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/gushi/biaoti_leixing.jsonl
        max_val_set: 250
    - _biaoti_mingju:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/gushi/biaoti_mingju.jsonl
        max_val_set: 250
    - _biaoti_zuozhe:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/gushi/biaoti_zuozhe.jsonl
        max_val_set: 250
    - _gushi-beijing_yiwen:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/gushi/gushi-beijing_yiwen.jsonl
        max_val_set: 250
    - _gushi_beijing:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/gushi/gushi_beijing.jsonl
        max_val_set: 250
    - _gushi_biaoti:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/gushi/gushi_biaoti.jsonl
        max_val_set: 250
    - _gushi_chaodai:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/gushi/gushi_chaodai.jsonl
        max_val_set: 250
    - _gushi_gushi:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/gushi/gushi_gushi.jsonl
        max_val_set: 250
    - _gushi_leixing:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/gushi/gushi_leixing.jsonl
        max_val_set: 250
    - _gushi_mingju:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/gushi/gushi_mingju.jsonl
        max_val_set: 250
    - _gushi_shangxi:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/gushi/gushi_shangxi.jsonl
        max_val_set: 250
    - _gushi_yiwen:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/gushi/gushi_yiwen.jsonl
        max_val_set: 250
    - _gushi_zuozhe:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/gushi/gushi_zuozhe.jsonl
        max_val_set: 250
    - _leixing_biaoti:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/gushi/leixing_biaoti.jsonl
        max_val_set: 250
    - _leixing_gushi:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/gushi/leixing_gushi.jsonl
        max_val_set: 250
    - _leixing_zuozhe:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/gushi/leixing_zuozhe.jsonl
        max_val_set: 250
    - _mingju_biaoti-yuanwen:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/gushi/mingju_biaoti-yuanwen.jsonl
        max_val_set: 250
    - _mingju_zuozhe:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/gushi/mingju_zuozhe.jsonl
        max_val_set: 250
    - _shiren_chaodai:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/gushi/shiren_chaodai.jsonl
        max_val_set: 250
    - _shiren_jieshao:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/gushi/shiren_jieshao.jsonl
        max_val_set: 250
    - _zuozhe-leixing_biaoti:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/gushi/zuozhe-leixing_biaoti.jsonl
        max_val_set: 250
    - _zuozhe_gushi:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/gushi/zuozhe_gushi.jsonl
        max_val_set: 250
    - _zuozhe_leixing:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/gushi/zuozhe_leixing.jsonl
        max_val_set: 250
    - _zuozhe_mingju:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/gushi/zuozhe_mingju.jsonl
        max_val_set: 250
    - _daan_miyu:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/miyu/daan_miyu.jsonl
        max_val_set: 250
    - _miyu_daan:
        val_split: 0.02
        input_file_path: /home/ubuntu/openassistant/data/datasets/zeroshot_data/miyu/miyu_daan.jsonl
        max_val_set: 250

defaults:
  rng_seed: 0xa1221f97
  learning_rate: 1e-5
  gradient_checkpointing: false
  gradient_accumulation_steps: 32
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_epsilon: 1e-12
  weight_decay: 0.00
  warmup_steps: 600
  eval_steps: 200
  save_strategy: steps
  save_steps: 1000
  max_length: 512
  val_max_length:
  num_train_epochs: 3
  logging_steps: 10
  max_grad_norm: 2.0
  save_total_limit: 4
  dtype: fp16
  eval_accumulation_steps:
  freeze_layer:
  datasets:
    - webgpt
    - squad_v2
    - adversarial_qa
    - trivia_qa_nocontext
    - xsum
    - cnn_dailymail
    - multi_news
    - scitldr
    - soda:
        input_max_length: 1024
    - joke
    - gsm8k
    - dive_mt
    - wmt2019_zh-en
    - wmt2019_ru-en
    - wmt2019_de-en
    - ted_trans_nl-en
    - ted_trans_de-ja
    - wmt2019_de-en
    - samsum
    - soda_dialogue
  # instructional_datasets:
  #  - humaneval_mbpp_codegen_qa
  #  - humaneval_mbpp_testgen_qa
  #  - grade_school_math_instructions
  #  - recipes
  #  - ubuntu_dialogue_qa
  #  - cmu_wiki_qa
  #  - youtube_subs_howto100M
  #  - iapp_wiki_qa_squad
  #  - zhihu-kol
  datasets_extra: [] # For config options to add additional datasets, since yaml doesn't let us extend arrays
  cache_dir: .cache
  loss_fn: CrossEntropyLoss
  eval_size:
  log_dir: "base"
  quantization: false
  seq2seqmodel: false
  poly_eps: 1.0
  fuse_gelu: true
  log_wandb: true
  samples_mixing: false # uses collator that mixes samples in the batch to create a single sample with possible multiple tasks within
  verbose: false
  output_dir: saved_model
  use_custom_sampler: false
  random_offset_probability: 0.8 # probability for random message offsets
  label_masking: true
  residual_dropout: 0.0
  use_flash_attention: false
  sort_by_length: false
  use_system_prefix: false
  system_prefix:
    "You are Joi, a large language model trained by Open-Assistant. Answer as
    concisely as possible.\nKnowledge cutoff: 2021-09-01\nCurrent date:
    2023-03-12"
  per_digit_tokens: false
  is_reward_model: false
  deepspeed_config: configs/zero_config.json

webgpt_dataset_only:
  datasets:
    - webgpt

per_digit_tokens:
  per_digit_tokens: true

math:
  datasets_extra: # Will get merged with datasets
    - minimath

pretrain:
  num_train_epochs: 1
  weight_decay: 0.0
  use_custom_sampler: true
  sort_by_length: false
  datasets:
    - joke:
        val_split: 0.05
    - webgpt:
        val_split: 0.05
        max_val_set: 250
    - gpt4all:
        val_split: 0.01
        max_val_set: 250
    - alpaca:
        val_split: 0.025
        max_val_set: 250
    - code_alpaca:
        val_split: 0.05
        max_val_set: 250
    - vicuna:
        max_val_set: 250
    - oig_file:
        source_url: https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl
        max_count: 10000
        min_length: 250
        val_split: 0.05
        max_val_set: 250
    - minimath:
        val_split: 0.05
    - humaneval_mbpp_codegen_qa:
        val_split: 0.05
    - humaneval_mbpp_testgen_qa:
        val_split: 0.05
    - grade_school_math_instructions:
        val_split: 0.05
    - recipes:
        val_split: 0.05
    - cmu_wiki_qa:
        val_split: 0.05
    - oa_wiki_qa_bart_10000row:
        val_split: 0.05
        max_val_set: 250
    - prosocial_dialogue:
        fraction: 0.1
        max_val_set: 250
    - explain_prosocial:
        fraction: 0.05
        max_val_set: 250
    - soda:
        fraction: 0.2
        max_val_set: 250
    - oa_leet10k:
        val_split: 0.05
        max_val_set: 250

oasst_only:
  save_strategy: epoch
  datasets:
    - oasst_export:
        lang: "bg,ca,cs,da,de,en,es,fr,hr,hu,it,nl,pl,pt,ro,ru,sl,sr,sv,uk"
        input_file_path: 2023-03-25_oasst_research_ready_synth_labels.jsonl.gz
        val_split: 0.05
  sort_by_length: false
  use_custom_sampler: false

reference-data:
  datasets:
    - oasst_export:
        lang: "bg,ca,cs,da,de,en,es,fr,hr,hu,it,nl,pl,pt,ro,ru,sl,sr,sv,uk"
        input_file_path: 2023-03-25_oasst_research_ready_synth_labels.jsonl.gz
        val_split: 0.05
    - alpaca
  sort_by_length: false
  use_custom_sampler: false

oasst_export_eu:
  save_strategy: epoch
  datasets:
    - oasst_export:
        lang: "en,es,de,fr"
        input_file_path: 2023-03-27_oasst_research_ready_synth.jsonl.gz
    - gpt4all
    - alpaca
    - code_alpaca
    - oig_file:
        source_url: https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl
        max_count: 10000
        min_length: 100
        val_split: 0.1
    - oig_file:
        source_url: https://huggingface.co/datasets/laion/OIG/raw/main/unified_grade_school_math_instructions.jsonl
        val_split: 0.1
        min_length: 100
  sort_by_length: false
  use_custom_sampler: false

oasst_export_latin_cyrillic:
  save_strategy: epoch
  datasets:
    - oasst_export:
        lang: "bg,ca,cs,da,de,en,es,fr,hr,hu,it,nl,pl,pt,ro,ru,sl,sr,sv,uk"
        input_file_path: 2023-03-27_oasst_research_ready_synth.jsonl.gz
    - alpaca
    - oig_file:
        source_url: https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl
        max_count: 10000
        min_length: 1000
        val_split: 0.2
    - oig_file:
        source_url: https://huggingface.co/datasets/laion/OIG/raw/main/unified_grade_school_math_instructions.jsonl
        val_split: 0.1
        min_length: 1000
  sort_by_length: false
  use_custom_sampler: false

reference-pythia-12b:
  dtype: fp16
  log_dir: "pythia_log_12b"
  learning_rate: 6e-6
  model_name: EleutherAI/pythia-12b-deduped
  output_dir: pythia_model_12b
  weight_decay: 0.0
  max_length: 2048
  warmup_steps: 100
  gradient_checkpointing: true
  gradient_accumulation_steps: 2
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  eval_steps: 100
  save_steps: 1000
  num_train_epochs: 8
  save_total_limit: 4

llama-7b:
  dtype: fp16
  log_dir: "llama_log_7b"
  learning_rate: 1e-5
  model_name: /home/ubuntu/llama_hf/7B
  output_dir: llama_model_7b
  weight_decay: 0.0
  max_length: 2048
  warmup_steps: 100
  gradient_checkpointing: true
  gradient_accumulation_steps: 2
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  eval_steps: 100
  save_steps: 500
  num_train_epochs: 8
  save_total_limit: 4
  use_flash_attention: false

llama-13b:
  dtype: fp16
  log_dir: "llama_log_13b"
  learning_rate: 1e-5
  model_name: /home/ubuntu/llama_hf/13B
  output_dir: llama_model_13b
  weight_decay: 0.0
  max_length: 2048
  warmup_steps: 100
  gradient_checkpointing: true
  gradient_accumulation_steps: 2
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  eval_steps: 500
  save_steps: 1000
  num_train_epochs: 8
  save_total_limit: 4
  use_flash_attention: false

llama-30b:
  dtype: fp16
  log_dir: "llama_log_30b"
  learning_rate: 2e-5
  model_name: /home/ubuntu/llama_hf/30B
  output_dir: llama_model_30b
  weight_decay: 0.0
  max_length: 512
  warmup_steps: 100
  gradient_checkpointing: true
  gradient_accumulation_steps: 16
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 5
  eval_steps: 100
  save_steps: 500
  num_train_epochs: 16
  save_total_limit: 4
  use_flash_attention: false

llama-30b-sft-6:
  dtype: fp16
  log_dir: "llama_log_30b"
  learning_rate: 1e-5
  #model_name: /home/ubuntu/Open-Assistant/model/model_training/.saved/llama-30b-super-pretrain/checkpoint-3500
  model_name: decapoda-research/llama-30b-hf
  output_dir: llama_model_30b
  deepspeed_config: configs/zero3_config_sft.json
  weight_decay: 0.0
  residual_dropout: 0.0
  max_length: 1024
  use_flash_attention: false
  warmup_steps: 20
  gradient_checkpointing: true
  gradient_accumulation_steps: 8
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  eval_steps: 101
  save_steps: 485
  num_train_epochs: 8
  save_total_limit: 3
  use_custom_sampler: true
  sort_by_length: false
  save_strategy: steps
  datasets:
    - oasst_export:
        lang: "bg,ca,cs,da,de,en,es,fr,hr,hu,it,nl,pl,pt,ro,ru,sl,sr,sv,uk"
        input_file_path: /home/ubuntu/openassistant/data/datasets/2023-04-12_oasst_all.trees.jsonl.gz
        val_split: 0.05
    # - vicuna:
    #     val_split: 0.05
    #     max_val_set: 800
    #     fraction: 0.8
    # - dolly15k:
    #     val_split: 0.05
    #     max_val_set: 300
    # - grade_school_math_instructions:
    #     val_split: 0.05
    # - code_alpaca:
    #     val_split: 0.05
    #     max_val_set: 250

llama-30b-pretrain:
  dtype: fp16
  log_dir: "llama_log_30b"
  learning_rate: 1e-5
  model_name: /home/ubuntu/llama_hf/30B
  output_dir: llama_model_30b
  deepspeed_config: configs/zero3_config_pretrain.json
  weight_decay: 0.0
  residual_dropout: 0.0
  max_length: 2048
  use_flash_attention: false
  warmup_steps: 100
  gradient_checkpointing: true
  gradient_accumulation_steps: 8
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 3
  eval_steps: 251
  save_steps: 500
  num_train_epochs: 1
  save_total_limit: 2

pythia-70m-deduped-sft-5:
  dtype: fp16
  log_dir: "llama_log_30b"
  learning_rate: 1e-5
  #model_name: /home/ubuntu/Open-Assistant/model/model_training/.saved/llama-30b-super-pretrain/checkpoint-3500
  model_name: EleutherAI/pythia-70m-deduped
  output_dir: pythia-70m-deduped
  deepspeed_config: configs/zero3_config_sft.json
  weight_decay: 0.0
  residual_dropout: 0.0
  max_length: 2048
  use_flash_attention: false
  warmup_steps: 20
  gradient_checkpointing: true
  gradient_accumulation_steps: 8
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 3
  eval_steps: 101
  save_steps: 485
  num_train_epochs: 8
  save_total_limit: 3
  use_custom_sampler: true
  sort_by_length: false
  save_strategy: steps
  datasets:
    - oasst_export:
        lang: "bg,ca,cs,da,de,en,es,fr,hr,hu,it,nl,pl,pt,ro,ru,sl,sr,sv,uk"
        input_file_path: /home/ubuntu/openassistant/data/datasets/2023-04-12_oasst_all.trees.jsonl.gz
        val_split: 0.05

pythia-70m-deduped:
  learning_rate: 8e-6
  # model_name: EleutherAI/pythia-1b-deduped
  model_name: EleutherAI/pythia-70m-deduped
  weight_decay: 0.0
  max_length: 520
  warmup_steps: 1000
  gradient_checkpointing: false
  gradient_accumulation_steps: 9
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 4
  output_dir: pythia_model
  datasets:
    - vicuna
    - soda

pythia-1B:
  learning_rate: 8e-6
  model_name: EleutherAI/pythia-1b-deduped
  weight_decay: 0.0
  max_length: 520
  warmup_steps: 10
  gradient_checkpointing: false
  gradient_accumulation_steps: 1
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 16

pythia-6.9B:
  learning_rate: 8e-6
  model_name: EleutherAI/pythia-6.9b-deduped
  weight_decay: 0.0
  max_length: 2048
  warmup_steps: 20
  gradient_checkpointing: false
  gradient_accumulation_steps: 2
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4

pythia-12B:
  learning_rate: 6e-6
  model_name: EleutherAI/pythia-12b-deduped
  weight_decay: 0.0
  max_length: 2048
  use_flash_attention: false
  warmup_steps: 100
  gradient_checkpointing: false
  gradient_accumulation_steps: 4
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 5
  eval_steps: 200
  save_steps: 500
  num_train_epochs: 16
  save_total_limit: 4

gpt-neox:
  model_name: EleutherAI/gpt-neox-20b
  deepspeed_config: configs/zero3_config_sft.json
  dtype: bf16
  learning_rate: 8e-6
  weight_decay: 0.0
  max_length: 1024
  warmup_steps: 1000
  eval_steps: 100
  gradient_checkpointing: true
  gradient_accumulation_steps: 2
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 8
  residual_dropout: 0.0
  use_flash_attention: false

galactica-125m:
  learning_rate: 5e-5
  model_name: facebook/galactica-125m
  weight_decay: 0.0
  warmup_steps: 600
  gradient_checkpointing: false
  gradient_accumulation_steps: 2
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4

gpt-jt:
  learning_rate: 8e-6
  model_name: togethercomputer/GPT-JT-6B-v1
  weight_decay: 0.0
  max_length: 1024
  warmup_steps: 600
  gradient_checkpointing: false
  gradient_accumulation_steps: 8
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4

cerebras_13B:
  learning_rate: 6e-6
  model_name: cerebras/Cerebras-GPT-13B
  weight_decay: 0.0
  max_length: 2048
  output_dir: cerebras_gpt_13b

cerebras_6.7B:
  learning_rate: 8e-6
  model_name: cerebras/Cerebras-GPT-6.7B
  weight_decay: 0.0
  max_length: 2048
  output_dir: cerebras_gpt_6_7b

codegen:
  learning_rate: 8e-6
  model_name: Salesforce/codegen-2B-multi
  weight_decay: 0.0
  max_length: 520
  warmup_steps: 1000
  gradient_checkpointing: false
  gradient_accumulation_steps: 9
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 4

debug:
  model_name: EleutherAI/pythia-70m-deduped
  eval_steps: 20
  eval_size: 20
  save_steps: 20
  gradient_accumulation_steps: 1
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  quantization: false
  log_wandb: false
  verbose: true
  num_train_epochs: 0.2
